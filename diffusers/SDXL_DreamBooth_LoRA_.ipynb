{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFNkvjgn1lPI"
      },
      "source": [
        "## Fine-tuning Stable Diffusion XL with DreamBooth and LoRA on a free-tier Colab Notebook üß®\n",
        "\n",
        "In this notebook, we show how to fine-tune [Stable Diffusion XL (SDXL)](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl) with [DreamBooth](https://huggingface.co/docs/diffusers/main/en/training/dreambooth) and [LoRA](https://huggingface.co/docs/diffusers/main/en/training/lora) on a T4 GPU.\n",
        "\n",
        "SDXL consists of a much larger UNet and two text encoders that make the cross-attention context quite larger than the previous variants.\n",
        "\n",
        "So, to pull this off, we will make use of several tricks such as gradient checkpointing, mixed-precision, and 8-bit Adam. So, hang tight and let's get started üß™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdM59WIoS01O"
      },
      "source": [
        "## Setup ü™ì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUxRrLfLMnBb",
        "outputId": "40eb7361-9659-48d9-a4f8-c90ab234e223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  1 20:21:07 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0             30W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check the GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "55U57alsRIek"
      },
      "outputs": [],
      "source": [
        "# Install dependencies.\n",
        "!pip install bitsandbytes transformers accelerate peft -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJXRjGZa1vFZ"
      },
      "source": [
        "Make sure to install `diffusers` from `main`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9atNWUXFZlVe",
        "outputId": "9f1954a9-73d9-4ec9-c5c0-bbbceb132732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2TRPaxGx3mE"
      },
      "source": [
        "Download diffusers SDXL DreamBooth training script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQpqmq7rx3mE",
        "outputId": "395ec6d0-2c4c-46d4-d216-7f4183cfbd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-01 20:21:47--  https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86775 (85K) [text/plain]\n",
            "Saving to: ‚Äòtrain_dreambooth_lora_sdxl.py.1‚Äô\n",
            "\n",
            "\r          train_dre   0%[                    ]       0  --.-KB/s               \rtrain_dreambooth_lo 100%[===================>]  84.74K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-12-01 20:21:47 (7.69 MB/s) - ‚Äòtrain_dreambooth_lora_sdxl.py.1‚Äô saved [86775/86775]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBcvYAyS8bU"
      },
      "source": [
        "## Dataset üê∂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UNkbXwj126Q"
      },
      "source": [
        "**Let's get our training data!**\n",
        "For this example, we'll download some images from the hub\n",
        "\n",
        "If you already have a dataset on the hub you wish to use, you can skip this part and go straight to: \"Prep for\n",
        "training üíª\" section, where you'll simply specify the dataset name.\n",
        "\n",
        "If your images are saved locally, and/or you want to add BLIP generated captions,\n",
        "pick option 1 or 2 below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvTS06eEx3mF"
      },
      "source": [
        "**Option 1:** upload example images from your local files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TqqhZ9R9x3mG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "c330a375-1321-43d3-8e2e-4a192f25fc8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: './ladles/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-842749640.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# pick a name for the image folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlocal_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./ladles/\"\u001b[0m \u001b[0;31m#@param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './ladles/'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# pick a name for the image folder\n",
        "local_dir = \"./ladles/\" #@param\n",
        "os.makedirs(local_dir)\n",
        "os.chdir(local_dir)\n",
        "\n",
        "# choose and upload local images into the newly created directory\n",
        "uploaded_images = files.upload()\n",
        "os.chdir(\"/content\") # back to parent directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3aCuZZGx3mG"
      },
      "source": [
        "**Option 2:** download example images from the hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BC36MOBdIWO"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import snapshot_download\n",
        "\n",
        "# local_dir = \"./dog/\"\n",
        "# snapshot_download(\n",
        "#     \"diffusers/dog-example\",\n",
        "#     local_dir=local_dir, repo_type=\"dataset\",\n",
        "#     ignore_patterns=\".gitattributes\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piJkbOe9OZbX"
      },
      "source": [
        "Preview the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0R-hByAPkIg"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "\n",
        "# def image_grid(imgs, rows, cols, resize=256):\n",
        "\n",
        "#     if resize is not None:\n",
        "#         imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "#     w, h = imgs[0].size\n",
        "#     grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "#     grid_w, grid_h = grid.size\n",
        "\n",
        "#     for i, img in enumerate(imgs):\n",
        "#         grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "#     return grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–µ—Å–ª–∏ —É —Ç–µ–±—è –µ—ë –Ω–µ—Ç –≤—ã—à–µ)\n",
        "def image_grid(imgs, rows, cols, resize=None):\n",
        "    if len(imgs) == 0:\n",
        "        raise ValueError(\"–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\")\n",
        "    if resize is not None:\n",
        "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "    w, h = imgs[0].size\n",
        "    import PIL.Image as ImageModule\n",
        "    grid = ImageModule.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i * w, 0))\n",
        "    return grid\n",
        "\n",
        "# —Å–æ–±–∏—Ä–∞–µ–º –ø—É—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏\n",
        "patterns = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\",\"*.JPG\",\"*.JPEG\",\"*.PNG\",\"*.WEBP\")\n",
        "paths = []\n",
        "for pat in patterns:\n",
        "    paths += glob.glob(os.path.join(LOCAL_DIR, pat))\n",
        "\n",
        "print(f\"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(paths)}\")\n",
        "if len(paths) == 0:\n",
        "    raise FileNotFoundError(f\"–ö–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ {LOCAL_DIR}\")\n",
        "\n",
        "# –æ—Ç–∫—Ä—ã–≤–∞–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5\n",
        "imgs = [Image.open(p).convert(\"RGB\") for p in paths]\n",
        "num_imgs_to_preview = min(5, len(imgs))\n",
        "image_grid(imgs[:num_imgs_to_preview], 1, num_imgs_to_preview, resize=256)\n"
      ],
      "metadata": {
        "id": "B94cnYg_dCKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mKSp6_wYcoWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqrxnDMUTADH"
      },
      "source": [
        "### Generate custom captions with BLIP\n",
        "Load BLIP to auto caption your images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZK1QuLqx3mH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load the processor and the captioning model\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\",torch_dtype=torch.float16).to(device)\n",
        "\n",
        "# captioning utility\n",
        "def caption_images(input_image):\n",
        "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    pixel_values = inputs.pixel_values\n",
        "\n",
        "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfkwE439x3mH"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "# create a list of (Pil.Image, path) pairs\n",
        "local_dir = \"./ladles/\"\n",
        "imgs_and_paths = [(path,Image.open(path)) for path in glob.glob(f\"{local_dir}*.jpeg\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V9cYeVDx3mH"
      },
      "source": [
        "Now let's add the concept token identifier (e.g. TOK) to each caption using a caption prefix.\n",
        "Feel free to change the prefix according to the concept you're training on!\n",
        "- for this example we can use \"a photo of TOK,\" other options include:\n",
        "    - For styles - \"In the style of TOK\"\n",
        "    - For faces - \"photo of a TOK person\"\n",
        "- You can add additional identifiers to the prefix that can help steer the model in the right direction.\n",
        "-- e.g. for this example, instead of \"a photo of TOK\" we can use \"a photo of TOK dog\" / \"a photo of TOK corgi dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJcZyeSVx3mH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "caption_prefix = \"a photo of skzladle steel ladle railcar, \" #@param\n",
        "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
        "  for img in imgs_and_paths:\n",
        "      caption = caption_prefix + caption_images(img[1]).split(\"\\n\")[0]\n",
        "      entry = {\"file_name\":img[0].split(\"/\")[-1], \"prompt\": caption}\n",
        "      json.dump(entry, outfile)\n",
        "      outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_5Skm_cx3mI"
      },
      "source": [
        "Free some memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4wfEW_cx3mI"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# delete the BLIP pipelines and free up some memory\n",
        "del blip_processor, blip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLptJV1vx3mI"
      },
      "source": [
        "## Prep for training üíª"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7OzF0i716Kf"
      },
      "source": [
        "Initialize `accelerate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW7eFPl4eYXy"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc0bxTr918es"
      },
      "source": [
        "### Log into your Hugging Face account\n",
        "Pass [your **write** access token](https://huggingface.co/settings/tokens) so that we can push the trained checkpoints to the Hugging Face Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Hf07sapecFY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-p7R70THc5"
      },
      "source": [
        "## Train! üî¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az-IQG-Ax3mJ"
      },
      "source": [
        "#### Set Hyperparameters ‚ö°\n",
        "To ensure we can DreamBooth with LoRA on a heavy pipeline like Stable Diffusion XL, we're using:\n",
        "\n",
        "* Gradient checkpointing (`--gradient_accumulation_steps`)\n",
        "* 8-bit Adam (`--use_8bit_adam`)\n",
        "* Mixed-precision training (`--mixed-precision=\"fp16\"`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZbz4IXe0bAn"
      },
      "source": [
        "### Launch training üöÄüöÄüöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIMwkmL7N82R"
      },
      "source": [
        "To allow for custom captions we need to install the `datasets` library, you can skip that if you want to train solely\n",
        " with `--instance_prompt`.\n",
        "In that case, specify `--instance_data_dir` instead of `--dataset_name`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTZuM7yNNZsT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Use `--output_dir` to specify your LoRA model repository name!\n",
        " - Use `--caption_column` to specify name of the cpation column in your dataset. In this example we used \"prompt\" to\n",
        " save our captions in the\n",
        " metadata file, change this according to your needs."
      ],
      "metadata": {
        "collapsed": false,
        "id": "fbri7qQ0mdyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOiZSXHfx3mJ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env bash\n",
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --instance_data_dir=\"/content/ladles\" \\\n",
        "  --caption_column=\"prompt\" \\\n",
        "  --instance_prompt=\"a photo of skzladle ladle railcar\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=3 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --max_train_steps=600 \\\n",
        "  --checkpointing_steps=200 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --seed=42 \\\n",
        "  --output_dir=\"lora-sdzxl-ladles\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11tgWz5x3mK"
      },
      "source": [
        "### Save your model to the hub and check it out üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uyy9n54EtBY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import whoami\n",
        "from pathlib import Path\n",
        "#@markdown make sure the `output_dir` you specify here is the same as the one used for training\n",
        "output_dir = \"lora-sdzxl-ladles\" #@param\n",
        "username = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\n",
        "repo_id = f\"{username}/{output_dir}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smLheRkvEQ9H"
      },
      "outputs": [],
      "source": [
        "# @markdown Sometimes training finishes succesfuly (i.e. a **.safetensores** file with the LoRA weights saved properly to your local `output_dir`) but there's not enough RAM in the free tier to push the model to the hub üôÅ\n",
        "# @markdown\n",
        "# @markdown To mitigate this, run this cell with your training arguments to make sure your model is uploaded! ü§ó\n",
        "\n",
        "# push to the hubüî•\n",
        "from train_dreambooth_lora_sdxl import save_model_card\n",
        "from huggingface_hub import upload_folder, create_repo\n",
        "\n",
        "repo_id = create_repo(repo_id, exist_ok=True).repo_id\n",
        "\n",
        "# change the params below according to your training arguments\n",
        "save_model_card(\n",
        "    repo_id = repo_id,\n",
        "    images=[],\n",
        "    base_model=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    train_text_encoder=False,  # –æ—Å—Ç–∞–≤—å –∫–∞–∫ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ\n",
        "    instance_prompt=\"a photo of skzladle ladle railcar\",  # <‚Äî –º–æ–π —Ç–æ–∫–µ–Ω + –∫–ª–∞—Å—Å\n",
        "    validation_prompt=None,\n",
        "    repo_folder=output_dir,\n",
        "    use_dora = False,\n",
        "    vae_path=\"madebyollin/sdxl-vae-fp16-fix\",\n",
        ")\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=output_dir,\n",
        "    commit_message=\"End of training\",\n",
        "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLoRqZGux3mK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "link_to_model = f\"https://huggingface.co/dariabalanuta24/lora-sdzxl-ladles\"\n",
        "display(Markdown(\"### Your model has finished training.\\nAccess it here: {}\".format(link_to_model)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvu9e6q_x3mK"
      },
      "source": [
        "Let's generate some images with it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH7-YJwMcyra"
      },
      "source": [
        "## Inference üêï"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "OUT_DIR = \"lora-sdzxl-ladles\"\n",
        "found = []\n",
        "for root, dirs, files in os.walk(OUT_DIR):\n",
        "    for f in files:\n",
        "        if f.endswith(\".safetensors\") or f.endswith(\".bin\"):\n",
        "            found.append(os.path.join(root, f))\n",
        "print(\"FOUND:\", *found, sep=\"\\n\")\n"
      ],
      "metadata": {
        "id": "Z6jG83GUhfbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTz6Zmfc0i-0"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "\n",
        "# vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "# pipe = DiffusionPipeline.from_pretrained(\n",
        "#     \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "#     vae=vae,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     variant=\"fp16\",\n",
        "#     use_safetensors=True\n",
        "# )\n",
        "# pipe.load_lora_weights(repo_id)\n",
        "# _ = pipe.to(\"cuda\")\n",
        "\n",
        "import torch, os\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "from huggingface_hub import hf_hub_download, list_repo_files\n",
        "\n",
        "repo_id = \"dariabalanuta24/lora-sdzxl-ladles\"   # —Ç–≤–æ–π repo_id\n",
        "local_output_dir = \"lora-sdzxl-ladles\"         # —Ç–≤–æ–π output_dir (–µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –ª–æ–∫–∞–ª—å–Ω–æ)\n",
        "\n",
        "# 1) –ë–∞–∑–∞ SDXL + VAE\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "# 2) –ü–æ–ø—Ä–æ–±—É–µ–º —É–º–Ω–æ –Ω–∞–π—Ç–∏ –≤–µ—Å–æ–≤–æ–π —Ñ–∞–π–ª –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏\n",
        "candidate_names = [\n",
        "    \"pytorch_lora_weights.safetensors\",\n",
        "    \"adapter_model.safetensors\",\n",
        "    \"checkpoint-600/pytorch_lora_weights.safetensors\",   # –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏–ª–æ—Å—å –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç–µ\n",
        "    \"checkpoint-400/pytorch_lora_weights.safetensors\",\n",
        "    \"checkpoint-200/pytorch_lora_weights.safetensors\",\n",
        "]\n",
        "\n",
        "loaded = False\n",
        "try:\n",
        "    files = list_repo_files(repo_id)\n",
        "    for name in candidate_names:\n",
        "        if name in files:\n",
        "            pipe.load_lora_weights(repo_id, weight_name=name)\n",
        "            print(f\"Loaded LoRA from Hub: {name}\")\n",
        "            loaded = True\n",
        "            break\n",
        "except Exception as e:\n",
        "    print(\"Hub listing failed, will try local folder. Error:\", e)\n",
        "\n",
        "# 3) –ï—Å–ª–∏ —Å Hub –Ω–µ –≤—ã—à–ª–æ ‚Äî –ø—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É output_dir\n",
        "if not loaded and os.path.isdir(local_output_dir):\n",
        "    for name in candidate_names:\n",
        "        path = os.path.join(local_output_dir, name)\n",
        "        if os.path.exists(path):\n",
        "            pipe.load_lora_weights(local_output_dir, weight_name=name)\n",
        "            print(f\"Loaded LoRA from local folder: {name}\")\n",
        "            loaded = True\n",
        "            break\n",
        "\n",
        "# 4) –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî –ø—Ä–æ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–º—è –≤ –∫–æ—Ä–Ω–µ –ª–æ–∫–∞–ª—å–Ω–æ\n",
        "if not loaded:\n",
        "    pipe.load_lora_weights(local_output_dir, weight_name=\"pytorch_lora_weights.safetensors\")\n",
        "    print(\"Loaded LoRA with default safetensors name from local folder.\")\n",
        "\n",
        "pipe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J1NsUP51E2w"
      },
      "outputs": [],
      "source": [
        "prompt = \"a photo of TOK dog in a bucket at the beach\" # @param\n",
        "\n",
        "image = pipe(prompt=prompt, num_inference_steps=25).images[0]\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}